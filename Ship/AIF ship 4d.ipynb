{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d090e8",
   "metadata": {},
   "source": [
    "# Active Inference Ship navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8840415",
   "metadata": {},
   "source": [
    "This notebooks covers fundamentals of the Active Inference framework implemented with the Bethe Free Energy optimisation with message passing on factor graphs. We use the mountain car problem as an example.\n",
    "\n",
    "- We refer reader to the [Thijs van de Laar (2019) \"Simulating active inference processes by message passing\"](https://doi.org/10.3389/frobt.2019.00020) original paper with more in-depth overview and explanation of the active inference agent implementation by message passing.\n",
    "- The original environment/task description is from [Ueltzhoeffer (2017) \"Deep active inference\"](https://arxiv.org/abs/1709.02341)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "c3b0c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/GitHub/RxInfer.jl/examples`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.activate(\".\"); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "b301ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using RxInfer, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac307f",
   "metadata": {},
   "source": [
    "## The mountain and physics \n",
    "\n",
    "For the purpose of this example we create a simple mountain valley with hard-coded physics, such that we do not depend on any external complex library. We have several configurable parameters for the experiment:\n",
    "- Engine-force limit\n",
    "- Tires friction coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "1118c8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ship_dynamic (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We need control rudder and accelerator for turning and accelerate. But at first, we only consider turning rate\n",
    "function ship_dynamic(engine_force_limit=0.1)\n",
    "    B =[ 0.0,0.0,1.0,0.0 ]\n",
    "    turning_rate=(a::Real) -> begin\n",
    "        B*engine_force_limit*tanh(a)\n",
    "    end\n",
    "    return (turning_rate)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "2951933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1244 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "turning_rate=ship_dynamic(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "531783b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " -0.0\n",
       "  0.0\n",
       " -0.1\n",
       "  3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_position = [-0.0,0.0,-0.1,3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "8b8b8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getAFromState (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function getAFromState(x,δt)\n",
    "    A=zeros(4)\n",
    "    A[1]=x[4]*cos(x[3])*δt\n",
    "    A[2]=x[4]*sin(x[3])*δt\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "45da3a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       "  2.9850124958340776\n",
       " -0.29950024994048446\n",
       "  0.0\n",
       "  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getAFromState(initial_position,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae812643",
   "metadata": {},
   "source": [
    "# World - agent interaction\n",
    "\n",
    "Because the states of the world are unknown to the agent, we wrap them in a comprehension. The comprehension returns only the functions for interacting with the world and not the hidden states. This way, we introduce a stateful world whose states cannot be directly observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "51b8c099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_world (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_world(;turning_rate, initial_position = [0.5,0.5,0.1,3.0])\n",
    "\n",
    "    y_t_min = initial_position\n",
    "    # y_dot_t_min = initial_velocity\n",
    "    \n",
    "    y_t = y_t_min\n",
    "    # y_dot_t = y_dot_t_min\n",
    "    \n",
    "    execute = (rudder::Float64) -> begin\n",
    "        # Compute next state\n",
    "        \n",
    "\n",
    "\n",
    "        # y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n",
    "        # y_dot_t = getAFromState() + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n",
    "        y_t = getAFromState(y_t_min,1) + y_t_min + turning_rate(rudder)\n",
    "    \n",
    "        # Reset state for next step\n",
    "        y_t_min = y_t\n",
    "        # y_dot_t_min = y_dot_t\n",
    "    end\n",
    "    \n",
    "    observe = () -> begin \n",
    "        return y_t\n",
    "    end\n",
    "        \n",
    "    return (execute, observe)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3ca36",
   "metadata": {},
   "source": [
    "## Naive approach\n",
    "\n",
    "In this simulation we are going to perform a naive action policy for tight full-power only. In this case, with limited engine power, the agent should not be able to achieve its goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "bbd21a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_naive  = 100 # Total simulation time\n",
    "pi_naive = 0.1 # Naive policy for right full-power only\n",
    "\n",
    "# Let there be a world\n",
    "(execute_naive, observe_naive) = create_world(; \n",
    "turning_rate = turning_rate,\n",
    "    initial_position = initial_position, \n",
    ");\n",
    "\n",
    "y_naive = Vector{Vector{Float64}}(undef, N_naive)\n",
    "for t = 1:N_naive\n",
    "    execute_naive(pi_naive) # Execute environmental process\n",
    "    y_naive[t] = observe_naive() # Observe external states\n",
    "end\n",
    "\n",
    "x_target=[100.0,100.0,0.0*π,3.0]\n",
    "\n",
    "animation_naive = @animate for i in 1:N_naive\n",
    "    # plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n",
    "    # scatter([y_naive[1][1]], [y_naive[1][2]])\n",
    "    # plot(y_naive[1][1],y_naive[1:i][2])\n",
    "    scatter([50], [50],xlims = (-100,100),ylims = (-100,100), label = \"Target\")\n",
    "    scatter!([y_naive[i][1]], [y_naive[i][2]],xlims = (-100,100),ylims = (-100,100),label = \"ship\")\n",
    "    plot!([y_naive[i][1],y_naive[i][1]+2*y_naive[i][4]*cos(y_naive[i][3])],[y_naive[i][2],y_naive[i][2]+2*y_naive[i][4]*sin(y_naive[i][3])],arrow=true,color=:black,label=\"\")\n",
    "    \n",
    "    # scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \n",
    "end\n",
    "\n",
    "gif(animation_naive, \"./ai-mountain-car-naive3.gif\", fps = 24, show_msg = false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a7393",
   "metadata": {},
   "source": [
    "[](ai-mountain-car-naive.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48e446",
   "metadata": {},
   "source": [
    "# Active inference approach\n",
    "\n",
    "In the active inference approach we are going to create an agent that models the environment around itself as well as the best possible actions in a probabilistic manner. That should help agent to understand that the brute-force approach is not the most efficient one and hopefully to realise that a little bit of swing is necessary to achieve its goal.\n",
    "\n",
    "The code in the next block defines the agent's internal beliefs over the external dynamics and its probabilistic model of the environment, which correspond accurately by directly using the functions defined above. We use the `@model` macro from `RxInfer` to define the probabilistic model and the `meta` block to define approximation methods for the nonlinear state-transition functions.\n",
    "\n",
    "In the model specification we in addition to the current state of the agent we include the beliefs over its future states (up to `T` steps ahead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "5a348e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1251 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hhh = (u::AbstractVector) -> turning_rate(u[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "69370c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1244 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "turning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "9c5d7902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#1253 (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh = (u::AbstractVector) -> turning_rate(u[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "d0c71ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function mountain_car(; T,turning_rate)\n",
    "    \n",
    "    # Transition function modeling transition due to gravity and friction\n",
    "    g = (s_t_min::AbstractVector) -> begin \n",
    "        s_t = similar(s_t_min) # Next state\n",
    "        s_t=getAFromState(s_t_min,1.0) + s_t_min\n",
    "        return s_t\n",
    "    end\n",
    "    \n",
    "    # Function for modeling engine control\n",
    "    h = (u::AbstractVector) -> turning_rate(u[1])\n",
    "    \n",
    "    # Inverse engine force, from change in state to corresponding engine force\n",
    "    # h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n",
    "    \n",
    "    # C = constvar([1. 0. 0. 0.;\n",
    "    #               0. 1. 0. 0.])\n",
    "\n",
    "    # Internal model perameters\n",
    "    Gamma = 1e4*diageye(4) # Transition precision\n",
    "    Theta = 1e-4*diageye(4) # Observation variance\n",
    "    \n",
    "    m_s_t_min = datavar(Vector{Float64})\n",
    "    V_s_t_min = datavar(Matrix{Float64})\n",
    "\n",
    "    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n",
    "    s_k_min = s_t_min\n",
    "    \n",
    "    m_u = datavar(Vector{Float64}, T)\n",
    "    V_u = datavar(Matrix{Float64}, T)\n",
    "    \n",
    "    m_x = datavar(Vector{Float64}, T)\n",
    "    V_x = datavar(Matrix{Float64}, T)\n",
    "    \n",
    "    u = randomvar(T)\n",
    "    s = randomvar(T)\n",
    "    x = randomvar(T)\n",
    "    \n",
    "    u_h_k = randomvar(T)\n",
    "    s_g_k = randomvar(T)\n",
    "    u_s_sum = randomvar(T)\n",
    "\n",
    "    for k in 1:T\n",
    "        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n",
    "        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Unscented()) }\n",
    "        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Unscented()) }\n",
    "        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n",
    "        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n",
    "        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n",
    "        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n",
    "        s_k_min = s[k]\n",
    "    end\n",
    "    \n",
    "    return (s, )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa15bc7",
   "metadata": {},
   "source": [
    "Because states of the agent are unknown to the world, we wrap them in a comprehension.\n",
    "The comprehension only returns functions for interacting with the agent.\n",
    "Internal beliefs cannot be directly observed, and interaction is only allowed through the Markov blanket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "89f81839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_agent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are going to use some private functionality from ReactiveMP, \n",
    "# in the future we should expose a proper API for this\n",
    "import RxInfer.ReactiveMP: getrecent, messageout\n",
    "\n",
    "function create_agent(; T = 20, turning_rate, x_target, initial_position)\n",
    "    Epsilon = fill(0.1, 1, 1)                # Control prior variance\n",
    "    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n",
    "    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n",
    "\n",
    "    Sigma    = 1e-4*diageye(4) # Goal prior variance\n",
    "    Sigma[3,3]=1e4\n",
    "    Sigma[4,4]=1e4\n",
    "    m_x      = [zeros(4) for k=1:T]\n",
    "    m_x[end]      = x_target\n",
    "    V_x      = [huge*diageye(4) for k=1:T]\n",
    "    V_x[end] = Sigma # Set prior to reach goal at t=T\n",
    "\n",
    "    # Set initial brain state prior\n",
    "    m_s_t_min = initial_position\n",
    "    V_s_t_min = tiny * diageye(4)\n",
    "    \n",
    "    # Set current inference results\n",
    "    result = nothing\n",
    "    pol = zeros(T)\n",
    "\n",
    "    # The `infer` function is the heart of the agent\n",
    "    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n",
    "    infer = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n",
    "        m_u[1] = [ upsilon_t ] # Register action with the generative model\n",
    "        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n",
    "\n",
    "        m_x[1] = y_hat_t # Register observation with the generative model\n",
    "        V_x[1] = tiny*diageye(4) # Clamp goal prior to observation\n",
    "\n",
    "        data = Dict(:m_u       => m_u, \n",
    "                    :V_u       => V_u, \n",
    "                    :m_x       => m_x, \n",
    "                    :V_x       => V_x,\n",
    "                    :m_s_t_min => m_s_t_min,\n",
    "                    :V_s_t_min => V_s_t_min)\n",
    "        \n",
    "        model  = mountain_car(; T = T, turning_rate) \n",
    "        result = inference(model = model, data = data)\n",
    "        # pol = [mode(result.posteriors[:u][k])[1] for k=1:T]\n",
    "    end\n",
    "    \n",
    "    # The `act` function returns the inferred best possible action\n",
    "    # act() = pol[1]\n",
    "    \n",
    "    act = () -> begin\n",
    "        if result !== nothing\n",
    "            return mode(result.posteriors[:u][3])[1] ##\n",
    "        else\n",
    "            return 0.0 # Without inference result we return some 'random' action\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # The `future` function returns the inferred future states\n",
    "    future = () -> begin \n",
    "        if result !== nothing \n",
    "            return getindex.(mode.(result.posteriors[:s]), 1)\n",
    "        else\n",
    "            return zeros(T)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n",
    "    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n",
    "    slide = () -> \n",
    "        (s, ) = result.returnval\n",
    "        \n",
    "        slide_msg_idx = 3 # This index is model dependend\n",
    "        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(s[2], slide_msg_idx)))\n",
    "\n",
    "        m_u = circshift(m_u, -1)\n",
    "        m_u[end] = [0.0]\n",
    "        # m_u = pol\n",
    "        V_u = circshift(V_u, -1)\n",
    "        V_u[end] = Epsilon\n",
    "\n",
    "        m_x = circshift(m_x, -1)\n",
    "        m_x[end] = x_target\n",
    "        V_x = circshift(V_x, -1)\n",
    "        V_x[end] = Sigma\n",
    "    end\n",
    "\n",
    "    return (infer, act, slide, future)    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "55699db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(execute_ai, observe_ai) = create_world(;\n",
    "    turning_rate, \n",
    "    initial_position = initial_position\n",
    "    # initial_velocity = initial_velocity\n",
    ") # Let there be a world\n",
    "\n",
    "T_ai = 50\n",
    "\n",
    "(infer_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n",
    "    T  = T_ai, \n",
    "    turning_rate=turning_rate,\n",
    "    x_target = x_target,\n",
    "    initial_position = initial_position\n",
    "    # initial_velocity = initial_velocity\n",
    ") \n",
    "\n",
    "N_ai = 100\n",
    "\n",
    "# Step through experimental protocol\n",
    "agent_a = Vector{Float64}(undef, N_ai) # Actions\n",
    "agent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\n",
    "agent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n",
    "pp=[]\n",
    "for t=1:N_ai\n",
    "    agent_a[t] = act_ai()            # Invoke an action from the agent\n",
    "    agent_f[t] = future_ai()         # Fetch the predicted future states\n",
    "    execute_ai(agent_a[t])           # The action influences hidden external states\n",
    "    agent_x[t] = observe_ai()        # Observe the current environmental outcome (update p)\n",
    "    infer_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n",
    "    slide_ai()                       # Prepare for next iteration\n",
    "end\n",
    "\n",
    "animation_ai = @animate for i in 1:N_ai\n",
    "    scatter([x_target[1]], [x_target[2]],xlims = (-200,200),ylims = (-200,200),label = \"Target\",markersize=6)\n",
    "    scatter!([agent_x[i][1]], [agent_x[i][2]],xlims = (-200,200),ylims = (-200,200),label = \"Ship\",markersize=6)\n",
    "    plot!([agent_x[i][1],agent_x[i][1]+3*agent_x[i][4]*cos(agent_x[i][3])],[agent_x[i][2],agent_x[i][2]+3*agent_x[i][4]*sin(agent_x[i][3])],arrow=true,color=:black,label=\"\")\n",
    "    \n",
    "    # scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \n",
    "end\n",
    "\n",
    "    \n",
    "gif(animation_ai, \"./ai-mountain-car-ai3.gif\", fps = 24, show_msg = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "402ce928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 100.0\n",
       " 100.0\n",
       "   0.0\n",
       "   3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "c25cea65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Float64}:\n",
       "   0.0\n",
       "   0.40890817645181016\n",
       "   0.46083486081341196\n",
       "   0.48050139089017263\n",
       "   0.4833337388408952\n",
       "   0.4838688947078241\n",
       "   0.48382289401810846\n",
       "   0.4831049645778421\n",
       "   0.48161162161434223\n",
       "   0.4792951242604246\n",
       "   ⋮\n",
       "  31.479219349463268\n",
       " -37.82533807000029\n",
       "   4.28719178364682\n",
       "   2.3465020457697703\n",
       "   0.37586424689976705\n",
       "  -8.523212622911952\n",
       "   1.733811179222834\n",
       "   4.012136740008533\n",
       " -39.843998570808175"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "5170dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.40890817645181016, 0.46083486081341196, 0.48050139089017263, 0.4833337388408952, 0.4838688947078241, 0.48382289401810846, 0.4831049645778421, 0.48161162161434223, 0.4792951242604246, 0.4761308079331681, 0.4721035919309567, 0.4672040719733284, 0.461427708202163, 0.4547756316801023, 0.4472542420863543, 0.4388757284903514, 0.4296579214827827, 0.4196240882167957, 0.40880304189427114, 0.39722840504729046, 0.3849387407685578, 0.3719773759421766, 0.3583907474780239, 0.3442296069062918, 0.3295472838937424, 0.3143997245979398, 0.29884441008545376, 0.2829407811909678, 0.26674898239442457, 0.25032970475153415, 0.23374448838523984, 0.21705486156453574, 0.20032249649435724, 0.18360945890389282, 0.1669797376732844, 0.1504976780071969, 0.13423212621148065, 0.11825517268432169, 0.10264697146695562, 0.08749722470826443, 0.07291192881241941, 0.059018864748384275, 0.045978181913036736, 0.03399251754941163, 0.0233200787849954, 0.014238202762994898, 0.00665105892469761, -0.006199092934473974, 0.0003883725652019789, 0.0006133266419250139, 0.006357387203744766, 0.01979129968995654, 0.0394939330428274, -0.06255074206265598, 0.4975395374991917, -5.964970651131395, -10.779484538293062, 2.100389628950492, 0.8159037877367379, -1.0506084334248378, 2.2579894147379953, 9.414966889057988, -8.963856095887225, 26.27789302899055, 7.0667050048581, -8.890012044659121, -9.142512846653464, 2.101781483167375, -1.9544348025246283, 0.267027256512701, 27.81124136954474, -0.2103522255934988, 9.174799717972354, 8.75419961917393, 0.8610570076614611, 31.81030547050721, -4.618160427094648, -2.016242808884023, -8.878758413914642, -0.4986017594888977, 0.7210047047482504, 0.9368796735658051, 2.1276754780913194, 1.2354166010620051, -1.518943155717095, -1.2916504438840426, 0.41422127453843155, 73.7566938600419, 7.275849560635157, -0.21385885143418903, 31.479219349463268, -37.82533807000029, 4.28719178364682, 2.3465020457697703, 0.37586424689976705, -8.523212622911952, 1.733811179222834, 4.012136740008533, -39.843998570808175]\n"
     ]
    }
   ],
   "source": [
    "println(agent_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "db3023eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 106.5485780937291\n",
       " 241.45725928705016\n",
       "   1.9947725480960878\n",
       "   3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execute_ai(agent_a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbff4a",
   "metadata": {},
   "source": [
    "As we can see the agent does indeed swing in order to reach its goal. Its interesting though that in the beginning the agent does not attempt to do that but only after some time has passed. That can be explained by the fact that we set `T_ai = 50`, which means that the agent must reach its goal after 50 time steps. In the beginning of the simulation this time horizon appears to be so far in the future that the agent decides not to do anything (in this way the Active Inference agent proved that procrastinating is smart!). After around `30` time steps the goal target becomes closer in time (agent has less than 20 time steps left to achieve the goal) and agent finally decides to act, predicts its future states and realises that in order to achieve its goal it must swing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
